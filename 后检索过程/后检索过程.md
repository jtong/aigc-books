
### 后检索过程

后检索过程的主要目标是有效整合检索到的上下文信息，以便生成更准确和相关的答案。这个过程包括以下几个方面：

1. 重新排序块

重新排序是指将检索到的文档块根据其与查询的相关性重新排列，以突出最相关的结果，减少整体文档池，为更精确的语言模型处理提供优化输入。常见的重新排序策略有：

- **基于规则的重新排序**：按照预定的规则对块进行排序，例如相关性得分、时间戳等。
- **基于模型的重新排序**：使用专门训练的模型对块进行排序，例如BERT系列的编码器-解码器模型（如SpanBERT）、专用重新排序模型如Cohere rerank或bge-raranker-large，以及通用大型语言模型如GPT。

2. 上下文选择与压缩

为了避免信息过载，将所有相关文档直接输入LLM可能会导致关键细节被稀释。上下文选择与压缩过程包括以下几个方面：

- **选择必要的信息**：从检索到的文档中挑选出最关键的部分，形成一个简洁的上下文提示。
- **强调关键部分**：确保提示中突出显示最重要的信息，以提高生成答案的质量和相关性。
- **缩短上下文长度**：通过删除不相关或冗余的内容，减小输入的长度，使LLM更专注于关键细节。

一些具体的方法有：

- **(Long) LLMLingua**：利用小型语言模型（SLM）如GPT-2 Small或LLaMA-7B检测和删除不重要的tokens，将其转化为一种对人类难以理解但LLM理解良好的形式。这种方法提供了一种直接实用的提示压缩方法，无需对LLM进行额外训练，同时平衡语言完整性和压缩比。
- **PRCA（增强适配检索器）**：通过训练信息提取器解决这个问题，确保选择最相关的信息。
- **RECOMP（对比学习信息压缩器）**：通过对比学习训练信息压缩器，每个训练数据点包括一个正样本和五个负样本，编码器在整个过程中通过对比损失进行训练。

3. 过滤与评估

除了压缩上下文，减少文档数量也有助于提高模型回答的准确性：

- **过滤-重新排序范式**：结合了LLM和SLM的优势。在这个范式中，SLM作为过滤器，而LLM作为重新排序代理。研究表明，指示LLM重新排序由SLM识别的难样本可显著改进各种信息提取（IE）任务的性能。
- **LLM批判过滤**：在生成最终答案前让LLM评估检索内容。这使得LLM能够通过批判性过滤掉相关性差的文档。例如，在Chatlaw中，通过提示LLM对引用的法律条文进行自我建议，以评估其相关性。
